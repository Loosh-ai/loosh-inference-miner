# Loosh Inference Miner Configuration
# Copy this file to .env and update with your values

# =============================================================================
# Network Configuration
# =============================================================================
NETUID=1
SUBTENSOR_NETWORK=local
SUBTENSOR_ADDRESS=ws://127.0.0.1:9945

# Network Configuration - Mainnet (uncomment to use)
#NETUID=78
#SUBTENSOR_NETWORK=finney
#SUBTENSOR_ADDRESS=wss://entrypoint-finney.opentensor.ai:443

# Network Configuration - Testnet (uncomment to use)
#NETUID=78
#SUBTENSOR_NETWORK=test
#SUBTENSOR_ADDRESS=wss://test.finney.opentensor.ai:443

# =============================================================================
# Wallet Configuration
# =============================================================================
# Note: Fiber only supports wallets in ~/.bittensor/wallets
WALLET_NAME=miner
HOTKEY_NAME=miner

# =============================================================================
# API Configuration
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000

# Axon port for Bittensor network communication
AXON_PORT=8089

# =============================================================================
# LLM Backend Configuration
# =============================================================================
# Backend to use: 'vllm', 'ollama', or 'llamacpp'
LLM_BACKEND=llamacpp

# Default model settings
DEFAULT_MODEL=mistralai/Mistral-7B-v0.1
DEFAULT_MAX_TOKENS=512
DEFAULT_TEMPERATURE=0.7
DEFAULT_TOP_P=0.95

# =============================================================================
# Backend-Specific Configuration
# =============================================================================

# llama.cpp Configuration
# Path to model file (for llama.cpp backend)
MODEL_PATH=

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=300.0

# vLLM Configuration
# vLLM server must be started separately before running the miner
# Models are automatically downloaded from HuggingFace when starting vLLM server
# Example: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-14B-Instruct --port 8000
VLLM_API_BASE=http://localhost:8000/v1

# =============================================================================
# GPU Configuration (for vLLM backend)
# =============================================================================
# Note: These settings are for reference - they should be passed to the vLLM server command
# The miner does not use these directly, but they should match your vLLM server configuration
# Number of GPU layers to use
TENSOR_PARALLEL_SIZE=1
# GPU memory utilization (0.0 to 1.0)
GPU_MEMORY_UTILIZATION=0.9
# Maximum context length
MAX_MODEL_LEN=4096

# =============================================================================
# Logging Configuration
# =============================================================================
LOG_LEVEL=INFO
LOG_FILE=

# =============================================================================
# Test Mode Configuration
# =============================================================================
# Enable test mode - returns success message without running inference
TEST_MODE=false

# =============================================================================
# Fiber MLTS Configuration
# =============================================================================
# Time-to-live for Fiber symmetric keys in seconds (default: 1 hour)
FIBER_KEY_TTL_SECONDS=3600
# Timeout for Fiber handshake operations in seconds
FIBER_HANDSHAKE_TIMEOUT_SECONDS=30
# Enable automatic key rotation for Fiber symmetric keys
FIBER_ENABLE_KEY_ROTATION=true

# =============================================================================
# Concurrency Configuration
# =============================================================================
# Maximum number of concurrent inference requests to process
# Default: 10
MAX_CONCURRENT_REQUESTS=10


