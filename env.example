# Loosh Inference Miner Configuration
# Copy this file to .env and update with your values

# =============================================================================
# Network Configuration
# =============================================================================
NETUID=1
SUBTENSOR_NETWORK=local
SUBTENSOR_ADDRESS=ws://127.0.0.1:9945

# Network Configuration - Mainnet (uncomment to use)
#NETUID=78
#SUBTENSOR_NETWORK=finney
#SUBTENSOR_ADDRESS=wss://entrypoint-finney.opentensor.ai:443

# Network Configuration - Testnet (uncomment to use)
#NETUID=78
#SUBTENSOR_NETWORK=test
#SUBTENSOR_ADDRESS=wss://test.finney.opentensor.ai:443

# =============================================================================
# Wallet Configuration
# =============================================================================
# Note: Fiber only supports wallets in ~/.bittensor/wallets
WALLET_NAME=miner
HOTKEY_NAME=miner

# =============================================================================
# API Configuration
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000

# Axon port for Bittensor network communication
AXON_PORT=8089

# =============================================================================
# LLM Backend Configuration
# =============================================================================
# Backend to use: 'vllm', 'ollama', or 'llamacpp'
LLM_BACKEND=llamacpp

# Default model settings
DEFAULT_MODEL=mistralai/Mistral-7B-v0.1
DEFAULT_MAX_TOKENS=512
DEFAULT_TEMPERATURE=0.7
DEFAULT_TOP_P=0.95

# =============================================================================
# Backend-Specific Configuration
# =============================================================================

# llama.cpp Configuration
# Path to model file (for llama.cpp backend)
MODEL_PATH=

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=300.0

# vLLM Configuration
# vLLM server must be started separately before running the miner
# Models are automatically downloaded from HuggingFace when starting vLLM server
# Example: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-14B-Instruct --port 8000
VLLM_API_BASE=http://localhost:8000/v1

# HuggingFace Cache Configuration
# By default, HuggingFace stores models in ~/.cache/huggingface/hub/
# For large models, you may want to use a different location with more disk space
# Set HF_HOME to change the base directory (models will be in $HF_HOME/hub/)
# Or set HUGGINGFACE_HUB_CACHE to directly specify the cache directory
# Example: HUGGINGFACE_HUB_CACHE=/mnt/large-disk/huggingface-cache
# HUGGINGFACE_HUB_CACHE=
# HF_HOME=

# =============================================================================
# GPU Configuration (for vLLM backend)
# =============================================================================
# Note: These settings are passed to the vLLM server when using run-miner.sh
# Number of GPUs to use for tensor parallelism
TENSOR_PARALLEL_SIZE=1
# GPU memory utilization (0.0 to 1.0)
GPU_MEMORY_UTILIZATION=0.9
# Maximum context length
MAX_MODEL_LEN=4096

# =============================================================================
# vLLM Advanced Configuration
# =============================================================================
# Enable automatic tool/function calling choice (true/false)
VLLM_ENABLE_AUTO_TOOL_CHOICE=true
# Tool call parser to use (hermes, functionary, mistral, or none)
VLLM_TOOL_CALL_PARSER=hermes
# Enable prefix caching for improved performance (true/false)
VLLM_ENABLE_PREFIX_CACHING=true
# Maximum number of sequences processed in parallel
VLLM_MAX_NUM_SEQS=64
# Maximum number of tokens in a batch
VLLM_MAX_NUM_BATCHED_TOKENS=32768

# =============================================================================
# Logging Configuration
# =============================================================================
LOG_LEVEL=INFO
LOG_FILE=

# =============================================================================
# Test Mode Configuration
# =============================================================================
# Enable test mode - returns success message without running inference
TEST_MODE=false

# =============================================================================
# Fiber MLTS Configuration
# =============================================================================
# Time-to-live for Fiber symmetric keys in seconds (default: 1 hour)
FIBER_KEY_TTL_SECONDS=3600
# Timeout for Fiber handshake operations in seconds
FIBER_HANDSHAKE_TIMEOUT_SECONDS=30
# Enable automatic key rotation for Fiber symmetric keys
FIBER_ENABLE_KEY_ROTATION=true

# =============================================================================
# Concurrency Configuration
# =============================================================================
# Maximum number of concurrent inference requests to process
# Default: 10
MAX_CONCURRENT_REQUESTS=10


